<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tutorial · PosDefManifoldML</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../"><img class="logo" src="../assets/logo.png" alt="PosDefManifoldML logo"/></a><h1>PosDefManifoldML</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">PosDefManifoldML Documentation</a></li><li class="current"><a class="toctext" href>Tutorial</a><ul class="internal"><li><a class="toctext" href="#Example-using-the-MDM-model-1">Example using the MDM model</a></li><li><a class="toctext" href="#Example-using-the-ENLR-model-1">Example using the ENLR model</a></li><li><a class="toctext" href="#Example-using-SVM-models-1">Example using SVM models</a></li></ul></li><li><a class="toctext" href="../MainModule/">Main Module</a></li><li><a class="toctext" href="../tools/">Tools</a></li><li><span class="toctext">ML Models: PD Manifold</span><ul><li><a class="toctext" href="../mdm/">Minimum Distance to Mean</a></li></ul></li><li><span class="toctext">ML Models: PD Tangent Space</span><ul><li><a class="toctext" href="../enlr/">Elastic-Net Logistic Regression</a></li><li><a class="toctext" href="../svm/">Support-Vector Machine</a></li></ul></li><li><a class="toctext" href="../cv/">fit, predict, cv</a></li><li><a class="toctext" href="../contribute/">How to contribute</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Tutorial</a></li></ul><a class="edit-page" href="https://github.com/Marco-Congedo/PosDefManifoldML.jl/blob/master/docs/src/tutorial.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Tutorial</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Tutorial-1" href="#Tutorial-1">Tutorial</a></h1><p>If you didn&#39;t, please read first the <a href="../#Overview-1">Overview</a>.</p><p><em>PosDefManifoldML</em> features two bacic <strong>pipelines</strong>:</p><p><strong>1)</strong> a machine learning (ML) model is first <strong>fit</strong> (trained), then it can be used to <strong>predict</strong> the <em>labels</em> of testing data or the <em>probability</em> of the data to belong to each class. The raw prediction function of the models is available as well.</p><p><strong>2)</strong> a <strong>k-fold cross-validation</strong> procedure allows to estimate directly the <strong>accuracy</strong> of ML models and compare them.</p><p>What <em>PosDefManifoldML</em> does for you is to allow an homogeneous syntax to run these two pipelines for all implemented ML models, it does not matter if they act directly on the manifold of positive definite matrices or on the tangent space. Furthermore, models acting on the tangent space can take as input Euclidean feature vectors instead of positive definite matrices, thus they can be used in many more situations.</p><p><strong>get data</strong></p><p>A real data example will be added soon. For now, let us create simulated data for a <strong>2-class example</strong>. First, let us create symmetric positive definite matrices (real positive definite matrices):</p><pre><code class="language-none">using PosDefManifoldML

PTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80);</code></pre><ul><li><code>PTr</code> is the simulated training set, holding 30 matrices for class 1 and 40 matrices for class 2</li><li><code>PTe</code> is the testing set, holding 60 matrices for class 1 and 80 matrices for class 2.</li><li><code>yTr</code> is a vector of 70 labels for the training set</li><li><code>yTe</code> is a vector of 140 labels for the testing set</li></ul><p>All matrices are of size 10x10.</p><h2><a class="nav-anchor" id="Example-using-the-MDM-model-1" href="#Example-using-the-MDM-model-1">Example using the MDM model</a></h2><p>The <strong>minimum distance to mean (MDM)</strong> classifier is an example of classifier acting directly on the manifold. It is deterministic and no hyperparameter tuning is requested.</p><h3><a class="nav-anchor" id="MDM-Pipeline-1.-(fit-and-predict)-1" href="#MDM-Pipeline-1.-(fit-and-predict)-1">MDM Pipeline 1. (fit and predict)</a></h3><p><strong>Craete and fit an MDM model</strong></p><p>An MDM model is created and fit with training data such as</p><pre><code class="language-none">m = fit(MDM(Fisher), PTr, yTr)</code></pre><p>where <code>Fisher</code> is the usual choice of a <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/MainModule/#Metric::Enumerated-type-1">Metric</a> as declared in the parent package <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/">PosDefManifold</a>.</p><p>Since the Fisher metric is the default (for all ML models), the above is equivalent to:</p><pre><code class="language-none">m = fit(MDM(), PTr, yTr)</code></pre><p>In order to adopt another metric:</p><pre><code class="language-none">m1 = fit(MDM(logEuclidean), PTr, yTr)</code></pre><p><strong>Predict (classify data)</strong></p><p>In order to predict the labels of unlabeled data (which we have stored in <code>PTe</code>), we invoke</p><pre><code class="language-none">yPred=predict(m, PTe, :l)</code></pre><p>The prediction error in percent can be retrived with</p><pre><code class="language-none">predictErr(yTe, yPred)</code></pre><p>or by</p><pre><code class="language-none">predictErr(yPred, yTe)</code></pre><p>where in <code>yTe</code> we have stored the <em>true</em> labels for the matrices in <code>PTe</code>.</p><p>If instead we wish to estimate the probabilities for the matrices in <code>PTe</code> of belonging to each class:</p><pre><code class="language-none">predict(m, PTe, :p)</code></pre><p>Finally, the output functions of the MDM are obtaine by (see <a href="../cv/#StatsBase.predict"><code>predict</code></a>)</p><pre><code class="language-none">predict(m, PTe, :f)</code></pre><h3><a class="nav-anchor" id="MDM-Pipeline-2.-(cross-validation)-1" href="#MDM-Pipeline-2.-(cross-validation)-1">MDM Pipeline 2. (cross-validation)</a></h3><p>The balanced accuracy estimated by a <em>k-fold cross-validation</em> is obtained such as (10-fold by default)</p><pre><code class="language-none">cv = cvAcc(MDM(), PTr, yTr)</code></pre><p>Struct <code>cv</code> has been created and therein you have access to average accuracy and confusion matrix as well as accuracies and confusion matrices for all folds. For example, print the average confusion matrix:</p><pre><code class="language-none">cv.avgCnf</code></pre><p>See <a href="../cv/#PosDefManifoldML.CVacc"><code>CVacc</code></a> for details on the fields of cross-validation objects.</p><h2><a class="nav-anchor" id="Example-using-the-ENLR-model-1" href="#Example-using-the-ENLR-model-1">Example using the ENLR model</a></h2><p>The <strong>elastic net logistic regression (ENLR)</strong> classifier is an example of classifier acting on the tangent space. Besides the <strong>metric</strong> (see above) used to compute a base-point for projecting the data onto the tangent space, it has a parameter <strong>alpha</strong> and an hyperparameter <strong>lambda</strong>. The <strong>alpha</strong> parameter allows to trade off between a pure <strong>ridge</strong> LR model (<span>$α=0$</span>) and a pure <strong>lasso</strong> LR model (<span>$α=1$</span>), which is the default. Given an alpha value, the model is fitted with a number of values for the <span>$λ$</span> (regularization) hyperparameter. Thus, differently from the previous example, tuning the <span>$λ$</span> hyperparameter is necessary.</p><p>Also, keep in mind that the <a href="../cv/#StatsBase.fit"><code>fit</code></a> and <a href="../cv/#StatsBase.predict"><code>predict</code></a> methods for ENLR models accept optional keyword arguments that are specific to this model.</p><p><strong>get data</strong></p><p>Let us get some simulated data (see the previous example for explanations).</p><pre><code class="language-none">PTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80);</code></pre><h3><a class="nav-anchor" id="ENLR-Pipeline-1.-(fit-and-predict)-1" href="#ENLR-Pipeline-1.-(fit-and-predict)-1">ENLR Pipeline 1. (fit and predict)</a></h3><p><strong>Craete and fit ENLR models</strong></p><p>By default, the Fisher metric ic adopted and a lasso model is fitted. The best value for the lambda hyperparameter is found by cross-validation:</p><pre><code class="language-none">m1 = fit(ENLR(), PTr, yTr; w=:balanced)</code></pre><p>Notice that since the class are unbalanced, with the <code>w=:balanced</code> argument (we may as well just use <code>w=:b</code>) we have requested to compute a balanced mean for projecting the matrices in <code>PTr</code> onto the tangent space.</p><p>The optimal value of lambda for this training data is</p><pre><code class="language-none">m1.best.lambda</code></pre><p>As in <em>GLMNet.jl</em>, the intercept and beta terms are retrived by</p><pre><code class="language-none">m1.best.a0
m1.best.betas</code></pre><p>The number of non-zero beta coefficients can be found for example by</p><pre><code class="language-none">length(unique(m1.best.betas))-1</code></pre><p>In order to fit a ridge LR model:</p><pre><code class="language-none">m2 = fit(ENLR(), PTr, yTr; w=:b, alpha=0)</code></pre><p>Values of <code>alpha</code> in range <span>$(0, 1)$</span> fit instead an elastic net LR model. In the following we also request not to standardize predictors:</p><pre><code class="language-none">m3 = fit(ENLR(Fisher), PTr, yTr; w=:b, alpha=0.9, standardize=false)</code></pre><p>In order to find the regularization path we use the <code>fitType</code> keyword argument:</p><pre><code class="language-none">m1 = fit(ENLR(Fisher), PTr, yTr; w=:b, fitType=:path)</code></pre><p>The values of lambda along the path are given by</p><pre><code class="language-none">m1.path.lambda</code></pre><p>We can also find the best value of the lambda hyperparameter and the regularization path at once, calling:</p><pre><code class="language-none">m1 = fit(ENLR(Fisher), PTr, yTr; w=:b, fitType=:all)</code></pre><p>For changing the metric see <a href="#MDM-Pipeline-1.-(fit-and-predict)-1">MDM Pipeline 1. (fit and predict)</a>.</p><p>See the documentation of the <a href="../cv/#StatsBase.fit"><code>fit</code></a> ENLR method for details on all available optional arguments.</p><p><strong>Classify data (predict)</strong></p><p>For prediction, we can request to use the best model (optimal lambda), to use a specific model of the regularization path or to use all the model in the regalurization path. Note that with the last call we have done here above both the <code>.best</code> and <code>.path</code> field of the <code>m1</code> structure have been created.</p><p>By default, prediction is obtained from the best model and we request to predict the labels:</p><pre><code class="language-none">yPred=predict(m1, PTe)

# prediction error in percent
predictErr(yPred, yTe)

# predict probabilities of matrices in `PTe` to belong to each class
predict(m1, PTe, :p)

# output function of the model for each class
predict(m1, PTe, :f)</code></pre><p>In order to request the predition of labels for all models in the regularization path:</p><pre><code class="language-none">yPred=predict(m1, PTe, :l, :path, 0)</code></pre><p>while for a specific model in the path (e.g., the 1Oth model):</p><pre><code class="language-none">yPred=predict(m1, PTe, :l, :path, 10)</code></pre><h3><a class="nav-anchor" id="ENLR-Pipeline-2.-(cross-validation)-1" href="#ENLR-Pipeline-2.-(cross-validation)-1">ENLR Pipeline 2. (cross-validation)</a></h3><p>The balanced accuracy estimated by a <em>k-fold cross-validation</em> is obtained with the exact same basic syntax for all models, with some specific optional keyword arguments for models acting in the tangent space, for example:</p><pre><code class="language-none">cv = cvAcc(ENLR(), PTr, yTr; w=:b)</code></pre><p>In order to perform another cross-validation arranging the training data differently in the folds:</p><pre><code class="language-none">cv = cvAcc(ENLR(), PTr, yTr; w=:b, shuffle=true)</code></pre><p>This last command can be invoked repeatedly.</p><h2><a class="nav-anchor" id="Example-using-SVM-models-1" href="#Example-using-SVM-models-1">Example using SVM models</a></h2><p>The SVM ML model actually encapsulates several <strong>support-vector classification</strong> and <strong>support-vector regression</strong> models. Here we are here concerned with the former, which include the <strong>C-Support Vector Classification (SVC)</strong>, the <strong>Nu-Support Vector Classification (NuSVC)</strong>, similar to SVC but using a parameter to control the number of support vectors, and the <strong>One-Class SVM (OneClassSVM)</strong>, which is used in general for unsupervised outlier detection. They all act in the tangent space like ENLR models. Besides the <strong>metric</strong> (see <a href="#MDM-Pipeline-1.-(fit-and-predict)-1">MDM Pipeline 1. (fit and predict)</a>) used to compute a base-point for projecting the data onto the tangent space and the type of SVM model (the <strong>svmType</strong>, = <code>SVC</code> (default), <code>NuSVC</code> or <code>OneClassSVM</code>), the main parameter is the <strong>kernel</strong>. Avaiable kernels are:</p><ul><li><code>RadialBasis</code> (default)</li><li><code>Linear</code></li><li><code>Polynomial</code></li><li><code>Sigmoid</code></li></ul><p>Several parameters are available for building all these kernels besides the linear one, which has no parameter. Lke for ENLR, for SVM models also an hyperparameter is to be found by cross-validation.</p><p><strong>get data</strong></p><p>Let us get some simulated data as in the previous examples.</p><pre><code class="language-none">PTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80);</code></pre><h3><a class="nav-anchor" id="SVM-Pipeline-1.-(fit-and-predict)-1" href="#SVM-Pipeline-1.-(fit-and-predict)-1">SVM Pipeline 1. (fit and predict)</a></h3><p><strong>Craete and fit SVM models</strong></p><p>By default, a C-Support Vector Classification model is fitted:</p><pre><code class="language-none">m1 = fit(SVM(), PTr, yTr; w=:b)</code></pre><p>Notice that as for the example above with for ENLR model, we have requested to compute a balanced mean for projecting the matrices in <code>PTr</code> onto the tangent space.</p><p>In order to fit a Nu-Support Vector Classification model:</p><pre><code class="language-none">m2 = fit(SVM(), PTr, yTr; w=:b, svmType=NuSVC)</code></pre><p>For using other kernels, e.g.:</p><pre><code class="language-none">m3 = fit(SVM(), PTr, yTr; w=:b, svmType=NuSVC, kernel=Linear)</code></pre><p>In the following we also request not to rescale predictors:</p><pre><code class="language-none">m3 = fit(SVM(), PTr, yTr;
        w=:b, svmType=NuSVC, kernel=Linear, rescale=())</code></pre><p>By default the Fisher metric is used. For changing it see <a href="#MDM-Pipeline-1.-(fit-and-predict)-1">MDM Pipeline 1. (fit and predict)</a>.</p><p>See the documentation of the <a href="../cv/#StatsBase.fit"><code>fit</code></a> ENLR method for details on all available optional arguments.</p><p><strong>Classify data (predict)</strong></p><p>Just the same as for the other models:</p><pre><code class="language-none">yPred=predict(m1, PTe)

# prediction error in percent
predictErr(yPred, yTe)

# predict probabilities of matrices in `PTe` to belong to each class
predict(m1, PTe, :p)

# output function of the model for each class
predict(m1, PTe, :f)</code></pre><h3><a class="nav-anchor" id="SVM-Pipeline-2.-(cross-validation)-1" href="#SVM-Pipeline-2.-(cross-validation)-1">SVM Pipeline 2. (cross-validation)</a></h3><p>Again, the balanced accuracy estimated by a <em>k-fold cross-validation</em> is obtained with the exact same basic syntax for all models, with some specific optional keyword arguments for models acting in the tangent space, for example:</p><pre><code class="language-none">cv = cvAcc(SVM(), PTr, yTr; w=:b)</code></pre><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">PosDefManifoldML Documentation</span></a><a class="next" href="../MainModule/"><span class="direction">Next</span><span class="title">Main Module</span></a></footer></article></body></html>
