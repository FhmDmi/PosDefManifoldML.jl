<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>fit, predict, cv ¬∑ PosDefManifoldML</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../"><img class="logo" src="../assets/logo.png" alt="PosDefManifoldML logo"/></a><h1>PosDefManifoldML</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">PosDefManifoldML Documentation</a></li><li><a class="toctext" href="../tutorial/">Tutorial</a></li><li><a class="toctext" href="../MainModule/">Main Module</a></li><li><a class="toctext" href="../tools/">Tools</a></li><li><span class="toctext">ML Models: PD Manifold</span><ul><li><a class="toctext" href="../mdm/">Minimum Distance to Mean</a></li></ul></li><li><span class="toctext">ML Models: PD Tangent Space</span><ul><li><a class="toctext" href="../enlr/">Elastic-Net Logistic Regression</a></li></ul></li><li class="current"><a class="toctext" href>fit, predict, cv</a><ul class="internal"></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>fit, predict, cv</a></li></ul><a class="edit-page" href="https://github.com/Marco-Congedo/PosDefManifoldML.jl/blob/master/docs/src/cv.md"><span class="fa">ÔÇõ</span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>fit, predict, cv</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="cv.jl-1" href="#cv.jl-1">cv.jl</a></h1><p>This unit implements <strong>cross-validation</strong> procedures for estimating the <strong>accuracy</strong> and <strong>balanced accuracy</strong> of machine learning models. It also reports the documentation of the <strong>fit</strong> and <strong>predict</strong> functions, as they are common to all models.</p><p><strong>Content</strong></p><table><tr><th style="text-align: left">struct</th><th style="text-align: left">description</th></tr><tr><td style="text-align: left"><a href="#PosDefManifoldML.CVacc"><code>CVacc</code></a></td><td style="text-align: left">encapsulate the result of cross-validation procedures for estimating accuracy</td></tr></table><table><tr><th style="text-align: left">function</th><th style="text-align: left">description</th></tr><tr><td style="text-align: left"><a href="#StatsBase.fit"><code>fit</code></a></td><td style="text-align: left">fit a model with training data, or create and fit it</td></tr><tr><td style="text-align: left"><a href="#GLMNet.predict"><code>predict</code></a></td><td style="text-align: left">preidct labels, probabilities or scoring functions on test data</td></tr><tr><td style="text-align: left"><a href="#PosDefManifoldML.cvAcc"><code>cvAcc</code></a></td><td style="text-align: left">estimate accuracy of a model by cross-validation</td></tr><tr><td style="text-align: left"><a href="#PosDefManifoldML.cvSetup"><code>cvSetup</code></a></td><td style="text-align: left">generate indexes for performing cross-validtions</td></tr></table><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="PosDefManifoldML.CVacc" href="#PosDefManifoldML.CVacc"><code>PosDefManifoldML.CVacc</code></a> ‚Äî <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">struct CVacc
    cvType    :: String
    scoring   :: Union{String, Nothing}
    modelType :: Union{String, Nothing}
    cnfs      :: Union{Vector{Matrix{T}}, Nothing} where T&lt;:Real
    avgCnf    :: Union{Matrix{T}, Nothing} where T&lt;:Real
    accs      :: Union{Vector{T}, Nothing} where T&lt;:Real
    avgAcc    :: Union{Real, Nothing}
    stdAcc    :: Union{Real, Nothing}
end</code></pre><p>A call to <a href="#PosDefManifoldML.cvAcc"><code>cvAcc</code></a> results in an instance of this structure. Fields:</p><p><code>.cvTpe</code> is the type of cross-validation technique, given as a string (e.g., &quot;10-kfold&quot;)</p><p><code>.scoring</code> is the type of accuracy that is computed, given as a string. This has been passed as argument to <a href="#PosDefManifoldML.cvAcc"><code>cvAcc</code></a>. Currently <em>accuracy</em> and <em>balanced accuracy</em> are supported.</p><p><code>.modelType</code> is type of the machine learning used for performing the cross-validation, given as a string.</p><p><code>.cnfs</code> is a vector of matrices holding the <em>confusion matrices</em> obtained at each fold of the cross-validation.</p><p><code>.avgCnf</code> is the <em>average confusion matrix</em> across the folds of the cross-validation.</p><p><code>.accs</code> is a vector of real numbers holding the <em>accuracies</em> obtained at each fold of the cross-validation.</p><p><code>.avgAcc</code> is the <em>average accuracy</em> across the folds of the cross-validation.</p><p><code>.stdAcc</code> is the <em>standard deviation of the accuracy</em> across the folds of the cross-validation.</p></div></div><a class="source-link" target="_blank" href="https://github.com/Marco-Congedo/PosDefManifoldML.jl/blob/77900b845b66438c11dab22226cf8382ea95afaf/src/cv.jl#L14-L56">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="StatsBase.fit" href="#StatsBase.fit"><code>StatsBase.fit</code></a> ‚Äî <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">function fit(model :: MDMmodel,
              ùêèTr   :: ‚ÑçVector,
              yTr   :: Vector;
       w        :: Vector = [],
       ‚úìw       :: Bool  = true,
       meanInit :: Union{‚ÑçVector, Nothing} = nothing,
       tol      :: Real  = 1e-7,
       verbose  :: Bool  = true,
       ‚è©       :: Bool  = true)</code></pre><p>Fit an <a href="../mdm/#PosDefManifoldML.MDM"><code>MDM</code></a> machine learning model, with training data <code>ùêèTr</code>, of type <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/MainModule/#%E2%84%8DVector-type-1">‚ÑçVector</a>, and corresponding labels <code>yTr</code>, of type <a href="../MainModule/#IntVector-1">IntVector</a>. Return the fitted model.</p><p>Labels must be provided using the natural numbers, i.e., <code>1</code> for the first class, <code>2</code> for the second class, etc.</p><p>Fitting an MDM model involves only computing a mean of all the matrices in each class. Those class means are computed according to the metric specified by the <a href="../mdm/#PosDefManifoldML.MDM"><code>MDM</code></a> constructor.</p><p>Optional keyword argument <code>w</code> is a vector of non-negative weights associated with the matrices in <code>ùêèTr</code>. This weights are used to compute the mean for each class. See method (3) of the <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/riemannianGeometry/#Statistics.mean">mean</a> function for the meaning of the arguments <code>w</code>, <code>‚úìw</code> and <code>‚è©</code>, to which they are passed. Keep in mind that here the weights should sum up to 1 separatedly for each class, which is what is ensured by this function if <code>‚úìw</code> is true.</p><p>Optional keyword argument <code>tol</code> is the tolerance required for those algorithms that compute the mean iteratively (they are those adopting the Fisher, logde0 or Wasserstein metric). For details on this argument see the functions that are called for computing the means:</p><ul><li>Fisher metric: <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/riemannianGeometry/#PosDefManifold.geometricMean">gmean</a></li><li>logdet0 metric: <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/riemannianGeometry/#PosDefManifold.logdet0Mean">ld0mean</a></li><li>Wasserstein metric: <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/riemannianGeometry/#PosDefManifold.wasMean">Wasmean</a>.</li></ul><p>For those algorithm an initialization can be provided with optional keyword argument <code>meanInit</code>. If provided, this must be a vector of <code>Hermitian</code> matrices of the <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/MainModule/#%F0%9D%95%84Vector-type-1">‚ÑçVector</a> type and must contain as many initializations as classes, in the natural order corresponding to the class labels (see above).</p><p>If <code>verbose</code> is true (default), information is printed in the REPL. This option is included to allow repeated calls to this function without crowding the REPL.</p><p><strong>See</strong>: <a href="../MainModule/#notation-and-nomenclature-1">notation &amp; nomenclature</a>, <a href="../MainModule/#the-‚ÑçVector-type-1">the ‚ÑçVector type</a>.</p><p><strong>See also</strong>: <a href="#GLMNet.predict"><code>predict</code></a>, <a href="#PosDefManifoldML.cvAcc"><code>cvAcc</code></a>.</p><p><strong>Examples</strong></p><pre><code class="language-none">using PosDefManifoldML

# generate some data
PTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80, 0.25)

# create and fit a model:
m=fit(MDM(Fisher), PTr, yTr)</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/Marco-Congedo/PosDefManifoldML.jl/blob/77900b845b66438c11dab22226cf8382ea95afaf/src/mdm.jl#L95-L163">source</a><div><div><pre><code class="language-none">function fit(model  :: ENLRmodel,
               ùêèTr  :: Union{‚ÑçVector, Matrix{Float64}},
               yTr  :: IntVector;
           w        :: Union{Symbol, Tuple, Vector} = [],
           meanISR  :: Union{‚Ñç, Nothing} = nothing,
		   meanInit :: Union{‚Ñç, Nothing} = nothing,
           fitType  :: Symbol = :best,
           vecRange :: UnitRange = ùêèTr isa ‚ÑçVector ? (1:size(ùêèTr[1], 2)) : (1:size(ùêèTr, 2)),
           verbose  :: Bool = true,
           ‚è©      :: Bool = true,
           # arguments for `GLMNet.glmnet` function
           alpha            :: Real = model.alpha==nothing ? 1.0 : model.alpha,
           weights          :: Vector{Float64} = ones(Float64, length(yTr)),
           intercept        :: Bool = true,
           standardize      :: Bool = alpha‚âà1.0 ? true : false,
           penalty_factor   :: Vector{Float64} = ones(Float64, _getDim(ùêèTr, vecRange)),
           constraints      :: Matrix{Float64} = [x for x in (-Inf, Inf), y in 1:_getDim(ùêèTr, vecRange)],
           offsets          :: Union{Vector{Float64}, Nothing} = nothing,
           dfmax            :: Int = _getDim(ùêèTr, vecRange),
           pmax             :: Int = min(dfmax*2+20, _getDim(ùêèTr, vecRange)),
           nlambda          :: Int = 100,
           lambda_min_ratio :: Real = (length(yTr) &lt; _getDim(ùêèTr, vecRange) ? 1e-2 : 1e-4),
           lambda           :: Vector{Float64} = Float64[],
           tol              :: Real = 1e-7,
           maxit            :: Int = 1000000,
           algorithm        :: Symbol = :newtonraphson,
           # selection method
           ŒªSelMeth :: Symbol = :sd1,
           # arguments for `GLMNet.glmnetcv` function
           nfolds   :: Int = min(10, div(size(yTr, 1), 3)),
           folds    :: Vector{Int} =
           begin
               n, r = divrem(size(yTr, 1), nfolds)
               shuffle!([repeat(1:nfolds, outer=n); 1:r])
           end,
           parallel ::Bool=false)
</code></pre><p>Create and fit an <a href="../enlr/#PosDefManifoldML.ENLR"><code>ENLR</code></a> machine learning model, with training data <code>ùêèTr</code>, of type <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/MainModule/#%E2%84%8DVector-type-1">‚ÑçVector</a>, and corresponding labels <code>yTr</code>, of type <a href="../MainModule/#IntVector-1">IntVector</a>. Return the fitted model.</p><p>As for all ML models acting in the tangent space, fitting an ENLR model involves computing a mean of all the matrices in <code>ùêèTr</code>, mapping all matrices onto the tangent space after parallel transporting them at the identity matrix and vectorizing them using the <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/riemannianGeometry/#PosDefManifold.vecP">vecP</a> operation. Once this is done, the elastic net logistic regression is fitted.</p><p>The mean is computed according to the <code>.metric</code> field of the <code>model</code>, with optional weights <code>w</code>. The <code>.metric</code> field of the <code>model</code> is passed to the <a href="../tools/#PosDefManifoldML.tsMap"><code>tsMap</code></a> function. By default the metric is the Fisher metric. See the examples here below to see how to change metric. See <a href="../mdm/#mdm.jl-1">mdm.jl</a> for the available metrics.</p><p><strong>Optional keyword arguments</strong></p><p>By default, uniform weights will be given to all observations for computing the mean to pass in the tangent space. This is equivalent to passing as argument <code>w=:uniform</code> (or <code>w=:u</code>). You can also pass as argument:</p><ul><li><code>w=:balanced</code> (or simply <code>w=:b</code>). If the two classes are unbalanced, the weights should be inversely proportional to the number of examples for each class, in such a way that each class contributes equally to the computation of the mean. This is equivalent of passing <code>w=tsWeights(yTr)</code>, see the <a href="../tools/#PosDefManifoldML.tsWeights"><code>tsWeights</code></a> function for details.</li><li><code>w=v</code>, where <code>v</code> is a user defined vector of non-negative weights for the observations, thus, <code>v</code> must contain the same number of elements as <code>yTr</code>. For example, <code>w=[1.0, 1.0, 2.0, 2.0, ...., 1.0]</code></li><li><code>w=t</code>, where <code>t</code> is a 2-tuple of real weights, one weight for each class, for example <code>w=(0.5, 1.5)</code>. This is equivalent to passing <code>w=tsWeights(yTr; classWeights=collect(t))</code>, see the <a href="../tools/#PosDefManifoldML.tsWeights"><code>tsWeights</code></a> function for details.</li></ul><p>If <code>meanISR</code> is passed as argument, the mean is not computed, instead this matrix is the inverse square root (ISR) of the mean used for projecting the matrices in the tangent space (see <a href="../tools/#PosDefManifoldML.tsMap"><code>tsMap</code></a>). Passed or computed, the inverse square root (ISR) of the mean will be written in the <code>.meanISR</code> field of the created <a href="../enlr/#PosDefManifoldML.ENLR"><code>ENLR</code></a> struct. If <code>meanISRis</code> is not provided and the <code>.metric</code> field of the <code>model</code> is Fisher, logdet0 or Wasserstein, the tolerance of the iterative algorithm used to compute the mean is set to the argument passed as <code>tol</code> (default 1e-7). Also, in this case a particular initialization for those iterative algorithms can be provided as an <code>Hermitian</code> matrix with argument <code>meanInit</code>.</p><p>This function also allows to fit a model passing as training data <code>ùêèTr</code> directly a matrix of feature vectors, where each feature vector is a row of the matrix. In this case the <code>metric</code> of the ENLR model and argument <code>meanISR</code> are not used. Therefore, the <code>.meanISR</code> field of the created <a href="../enlr/#PosDefManifoldML.ENLR"><code>ENLR</code></a> struct will be set to <code>nothing</code>.</p><p>If <code>fitType</code> = <code>:best</code> (default), a cross-validation procedure is run to find the best lambda hyperparameter for the training data. This finds a single model that is written into the <code>.best</code> field of the model that will be created.</p><p>If <code>fitType</code> = <code>:path</code>, the regularization path for several values of the lambda hyperparameter if found for the training data. This creates several models, which are written into the <code>.path</code> field of the model that will be created, none of which is optimal, in the cross-validation sense, for the training data.</p><p>If <code>fitType</code> = <code>:all</code>, both the above fits are performed and all fields of the model that will be created will be filled in.</p><p>If a <code>UnitRange</code> is passed with optional keyword argument <code>vecRange</code>, then if <code>ùêèTr</code> is a vector of <code>Hermitian</code>matrices, the vectorization of those matrices once they are projected onto the tangent space concerns only the rows (or columns) given in the specified range, else if <code>ùêèTr</code> is a matrix of feature vectors in the rows, then only the columns of <code>ùêèTr</code> given in the specified range will be used.</p><p>If <code>verbose</code> is true (default), information is printed in the REPL. This option is included to allow repeated calls to this function without crowding the REPL.</p><p>The <code>‚è©</code> argument (true by default) is passed to the <a href="../tools/#PosDefManifoldML.tsMap"><code>tsMap</code></a> function for projecting the matrices in <code>ùêèTr</code> onto the tangent space using multi-threading.</p><p>The remaining optional keyword arguments, are</p><ul><li><p>the arguments passed to the <code>GLMNet.glmnet</code> function for fitting the models. Those are always used.</p></li><li><p>the <code>ŒªSelMeth</code> argument and the arguments passed to the <code>GLMNet.glmnetcv</code> function for finding the best lambda hyperparamater by cross-validation. Those are used only if <code>fitType</code> = <code>:path</code> or = <code>:all</code>.</p></li></ul><p><strong>Optional keyword arguments for fitting the model(s) using GLMNet</strong></p><p><code>alpha</code>: the hyperparameter in <span>$[0, 1]$</span> to trade-off an elestic-net model. <span>$Œ±=0$</span> requests a pure <em>ridge</em> model and <span>$Œ±=1$</span> a pure <em>lasso</em> model. This defaults to 1.0, which specifies a lasso model, unless the input <a href="../enlr/#PosDefManifoldML.ENLR"><code>ENLR</code></a> <code>model</code> has another value in the <code>alpha</code> field, in which case this value is used. If argument <code>alpha</code> is passed here, it will overwrite the <code>alpha</code> field of the input <code>model</code>.</p><p><code>weights</code>: a vector of weights for each matrix (or feature vectors) of the same size as <code>yTr</code>. It defaults to 1 for all matrices.</p><p><code>intercept</code>: whether to fit an intercept term. The intercept is always unpenalized. Defaults to true.</p><p><code>standardize</code>: whether to standardize predictors so that they are in the same units. Differently from GLMNet.jl, by default this is true for lasso models (<span>$Œ±=1$</span>), false otherwise (<span>$0‚â§Œ±&lt;1$</span>).</p><p><code>penalty_factor</code>: a vector of length <span>$n(n+1)/2$</span>, where <span>$n$</span> is the dimension of the original PD matrices on which the model is applied, of penalties for each predictor in the tangent vectors. This defaults to all ones, which weights each predictor equally. To specify that a predictor should be unpenalized, set the corresponding entry to zero.</p><p><code>constraints</code>: an <span>$[n(n+1)/2]$</span> x <span>$2$</span> matrix specifying lower bounds (first column) and upper bounds (second column) on each predictor. By default, this is [-Inf Inf] for each predictor (each element of tangent vectors).</p><p><code>dfmax</code>: The maximum number of predictors in the largest model.</p><p><code>pmax</code>: The maximum number of predictors in any model.</p><p><code>nlambda</code>: The number of values of <span>$Œª$</span> along the path to consider.</p><p><code>lambda_min_ratio</code>: The smallest <span>$Œª$</span> value to consider, as a ratio of the value of <span>$Œª$</span> that gives the null model (<em>i.e.</em>, the model with only an intercept). If the number of observations exceeds the number of variables, this defaults to 0.0001, otherwise 0.01.</p><p><code>lambda</code>: The <span>$Œª$</span> values to consider for fitting. By default, this is determined from <code>nlambda</code> and <code>lambda_min_ratio</code>.</p><p><code>tol</code>: Convergence criterion. Defaults to 1e-7.</p><p><code>maxit</code>: The maximum number of iterations of the cyclic coordinate descent algorithm. If convergence is not achieved, a warning is returned.</p><p><code>algorithm</code>: the algorithm used to find the regularization path. Possible values are <code>:newtonraphson</code> (default) and <code>:modifiednewtonraphson</code>.</p><p>For further informations on those arguments, refer to the resources on the GLMNet package <a href="../#-1">üéì</a>.</p><p><strong>Optional Keyword arguments for finding the best model by cv</strong></p><p><code>ŒªSelMeth</code> = <code>:sd1</code> (default), the best model is defined as the one allowing the highest <code>cvŒª.meanloss</code> within one standard deviation of the minimum, otherwise it is defined as the one allowing the minimum <code>cvŒª.meanloss</code>. Note that in selecting a model, the model with only the intercept term, if it exists, is ignored. See <a href="../enlr/#PosDefManifoldML.ENLRmodel"><code>ENLRmodel</code></a> for a description of the <code>.cvŒª</code> field of the model structure.</p><p>Arguments <code>nfolds</code>, <code>folds</code> and <code>parallel</code> are passed to the <code>GLMNet.glmnetcv</code> function. Please refer to the resources on GLMNet for details <a href="../#-1">üéì</a>.</p><p><strong>See</strong>: <a href="../MainModule/#notation-and-nomenclature-1">notation &amp; nomenclature</a>, <a href="../MainModule/#the-‚ÑçVector-type-1">the ‚ÑçVector type</a>.</p><p><strong>See also</strong>: <a href="#GLMNet.predict"><code>predict</code></a>, <a href="#PosDefManifoldML.cvAcc"><code>cvAcc</code></a>.</p><p><strong>Examples</strong></p><pre><code class="language-none">using PosDefManifoldML

# generate some data
PTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80, 0.1)

# Fit an ENLR lasso model and find the best model by cross-validation:
m=fit(ENLR(), PTr, yTr)

# ... balancing the weights for tangent space mapping
m=fit(ENLR(), PTr, yTr; w=tsWeights(yTr))

# ... using the log-Eucidean metric for tangent space projection
m=fit(ENLR(logEuclidean), PTr, yTr)

# Fit an ENLR ridge model and find the best model by cv:
m=fit(ENLR(Fisher), PTr, yTr; alpha=0)

# Fit an ENLR elastic-net model (Œ±=0.9) and find the best model by cv:
m=fit(ENLR(Fisher), PTr, yTr; alpha=0.9)

# Fit an ENLR lasso model and its regularization path:
m=fit(ENLR(), PTr, yTr; fitType=:path)

# Fit an ENLR lasso model, its regularization path
# and the best model found by cv:
m=fit(ENLR(), PTr, yTr; fitType=:all)
</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/Marco-Congedo/PosDefManifoldML.jl/blob/77900b845b66438c11dab22226cf8382ea95afaf/src/enlr.jl#L190-L437">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GLMNet.predict" href="#GLMNet.predict"><code>GLMNet.predict</code></a> ‚Äî <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">function predict(model  :: MDMmodel,
                 ùêèTe    :: ‚ÑçVector,
                 what   :: Symbol = :labels;
               verbose :: Bool = true,
               ‚è©     :: Bool = true)</code></pre><p>Given an <a href="../mdm/#PosDefManifoldML.MDM"><code>MDM</code></a> <code>model</code> trained (fitted) on <span>$z$</span> classes and a testing set of <span>$k$</span> positive definite matrices <code>ùêèTe</code> of type <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/MainModule/#%E2%84%8DVector-type-1">‚ÑçVector</a>,</p><p>if <code>what</code> is <code>:labels</code> or <code>:l</code> (default), return the predicted <strong>class labels</strong> for each matrix in <code>ùêèTe</code>, as an <a href="../MainModule/#IntVector-1">IntVector</a>. For MDM models, the predicted class &#39;label&#39; of an unlabeled matrix is the serial number of the class whose mean is the closest to the matrix (minimum distance to mean). The labels are &#39;1&#39; for class 1, &#39;2&#39; for class 2, etc;</p><p>if <code>what</code> is <code>:probabilities</code> or <code>:p</code>, return the predicted <strong>probabilities</strong> for each matrix in <code>ùêèTe</code> to belong to a all classes, as a <span>$k$</span>-vector of <span>$z$</span> vectors holding reals in <span>$[0, 1]$</span> (probabilities). The &#39;probabilities&#39; are obtained passing to a <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax function</a> minus the squared distances of each unlabeled matrix to all class means;</p><p>if <code>what</code> is <code>:f</code> or <code>:functions</code>, return the <strong>output function</strong> of the model. The ratio of the squared distance to all classes to their geometric mean gives the &#39;functions&#39;.</p><p>If <code>verbose</code> is true (default), information is printed in the REPL. This option is included to allow repeated calls to this function without crowding the REPL.</p><p>It f <code>‚è©</code> is true (default), the computation of distances is multi-threaded.</p><p><strong>See</strong>: <a href="../MainModule/#notation-and-nomenclature-1">notation &amp; nomenclature</a>, <a href="../MainModule/#the-‚ÑçVector-type-1">the ‚ÑçVector type</a>.</p><p><strong>See also</strong>: <a href="#StatsBase.fit"><code>fit</code></a>, <a href="#PosDefManifoldML.cvAcc"><code>cvAcc</code></a>, <a href="../tools/#PosDefManifoldML.predictErr"><code>predictErr</code></a>.</p><p><strong>Examples</strong></p><pre><code class="language-none">using PosDefManifoldML

# generate some data
PTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80)

# craete and fit an MDM model
m=fit(MDM(Fisher), PTr, yTr)

# predict labels
yPred=predict(m, PTe, :l)

# prediction error
predErr=predictErr(yTe, yPred)

# predict probabilities
predict(m, PTe, :p)

# output functions
predict(m, PTe, :f)
</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/Marco-Congedo/PosDefManifoldML.jl/blob/77900b845b66438c11dab22226cf8382ea95afaf/src/mdm.jl#L213-L278">source</a><div><div><pre><code class="language-none">function predict(model   :: ENLRmodel,
                 ùêèTe     :: Union{‚ÑçVector, Matrix{Float64}},
                 what    :: Symbol = :labels,
                 fitType :: Symbol = :best,
                 onWhich :: Int    = Int(fitType==:best);
			transfer   :: Union{‚Ñç, Nothing} = nothing,
            vecRange   :: UnitRange = ùêèTe isa ‚ÑçVector ? (1:size(ùêèTe[1], 2)) : (1:size(ùêèTe, 2)),
            checks     :: Bool = true,
            verbose    :: Bool = true,
            ‚è©        :: Bool = true)</code></pre><p>Given an <a href="../enlr/#PosDefManifoldML.ENLR"><code>ENLR</code></a> <code>model</code> trained (fitted) on 2 classes and a testing set of <span>$k$</span> positive definite matrices <code>ùêèTe</code> of type <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/MainModule/#%E2%84%8DVector-type-1">‚ÑçVector</a>,</p><p>if <code>what</code> is <code>:labels</code> or <code>:l</code> (default), return the predicted <strong>class labels</strong> for each matrix in <code>ùêèTe</code>, as an <a href="../MainModule/#IntVector-1">IntVector</a>. Those labels are &#39;1&#39; for class 1 and &#39;2&#39; for class 2;</p><p>if <code>what</code> is <code>:probabilities</code> or <code>:p</code>, return the predicted <strong>probabilities</strong> for each matrix in <code>ùêèTe</code> to belong to a all classes, as a <span>$k$</span>-vector of <span>$z$</span> vectors holding reals in <span>$[0, 1]$</span> (probabilities). The &#39;probabilities&#39; are obtained passing to a <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax function</a> the output of the ENLR model and zero;</p><p>if <code>what</code> is <code>:f</code> or <code>:functions</code>, return the <strong>output function</strong> of the model, which is the raw output of the ENLR model.</p><p>If <code>fitType</code> = <code>:best</code> (default), the best model that has been found by cross-validation is used for prediction.</p><p>If <code>fitType</code> = <code>:path</code>,</p><ul><li><p>if <code>onWhich</code> is a valid serial number for a model in the <code>model.path</code>, this model is used for prediction,</p></li><li><p>if <code>onWhich</code> is zero, all model in the <code>model.path</code> will be used for predictions, thus the output will be multiplied by the number of models in <code>model.path</code>.</p></li></ul><p>Argumet <code>onWhich</code> has no effect if <code>fitType</code> = <code>:best</code>.</p><div class="admonition note"><div class="admonition-title">Nota Bene</div><div class="admonition-text"><p>By default, the <a href="#StatsBase.fit"><code>fit</code></a> function fits only the <code>best</code> model. If you want to use the <code>fitType</code> = <code>:path</code> option you need to invoke the fit function with optional keyword argument <code>fitType</code>=<code>:path</code> or <code>fitType</code>=<code>:all</code>. See the <a href="#StatsBase.fit"><code>fit</code></a> function for details.</p></div></div><p>Option keyword argument <code>transfer</code> can be used to specify the principal inverse square root (ISR) of a new mean to be used as base point for projecting the matrices in <code>ùêèTe</code> onto the tangent space. By default <code>transfer</code> is equal to nothing, implying that the base point will be the mean used to fit the model. Passing a new mean ISR allows the <em>adaptation</em> first described in Barachant et <em>al.</em>(2013). Typically <code>transfer</code> is the ISR of the mean of the matrices in <code>ùêèTe</code> or of a subset of them. Notice that this actually performs <em>transfer learning</em> by parallel transporting both the training and test data to the identity matrix as defined in Zanini et <em>al.</em>(2018) and taken up in Rodrigues et <em>al.</em>(2019)<a href="../#-1">üéì</a>.</p><p>Optional keyword argument <code>vecRange</code> has the same meaning as in the <a href="#StatsBase.fit"><code>fit</code></a> function. In general, if you have used it to fit the model youwill use the same here.</p><p>If <code>checks</code> is true (default), checks on the validity of the arguments are performed. This can be set to false to spped up computations.</p><p>If <code>verbose</code> is true (default), information is printed in the REPL. This option is included to allow repeated calls to this function without crowding the REPL.</p><p>If ‚è© = true (default) and <code>ùêèTe</code> is an ‚ÑçVector type, the projection onto the tangent space is multi-threaded.</p><p><strong>See</strong>: <a href="../MainModule/#notation-and-nomenclature-1">notation &amp; nomenclature</a>, <a href="../MainModule/#the-‚ÑçVector-type-1">the ‚ÑçVector type</a>.</p><p><strong>See also</strong>: <a href="#StatsBase.fit"><code>fit</code></a>, <a href="#PosDefManifoldML.cvAcc"><code>cvAcc</code></a>, <a href="../tools/#PosDefManifoldML.predictErr"><code>predictErr</code></a>.</p><p><strong>Examples</strong></p><pre><code class="language-none">using PosDefManifoldML

# generate some data
PTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80)

# fit an ENLR lasso model and find the best model by cv
m=fit(ENLR(Fisher), PTr, yTr)

# predict labels from the best model
yPred=predict(m, PTe, :l)
# prediction error
predErr=predictErr(yTe, yPred)

# predict probabilities from the best model
predict(m, PTe, :p)

# output functions from the best model
predict(m, PTe, :f)

# fit a regularization path for an ENLR lasso model
m=fit(ENLR(Fisher), PTr, yTr; fitType=:path)

# predict labels using a specific model
yPred=predict(m, PTe, :l, :path, 10)

# predict labels for all models
yPred=predict(m, PTe, :l, :path, 0)
# prediction error for all models
predErr=[predictErr(yTe, yPred[:, i]) for i=1:size(yPred, 2)]

# predict probabilities from a specific model
predict(m, PTe, :p, :path, 12)

# predict probabilities from all models
predict(m, PTe, :p, :path, 0)

# output functions from specific model
predict(m, PTe, :f, :path, 3)

# output functions for all models
predict(m, PTe, :f, :path, 0)
</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/Marco-Congedo/PosDefManifoldML.jl/blob/77900b845b66438c11dab22226cf8382ea95afaf/src/enlr.jl#L568-L695">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="PosDefManifoldML.cvAcc" href="#PosDefManifoldML.cvAcc"><code>PosDefManifoldML.cvAcc</code></a> ‚Äî <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">function cvAcc(model   :: MLmodel,
               ùêèTr     :: ‚ÑçVector,
               yTr     :: IntVector;
           tol       :: Real      = 0.,
           nFolds    :: Int       = min(10, length(yTr)√∑3),
           scoring   :: Symbol    = :b,
           shuffle   :: Bool      = false,
           vecRange  :: UnitRange = ùêèTr isa ‚ÑçVector ? (1:size(ùêèTr[1], 2)) : (1:size(ùêèTr, 2)),
           verbose   :: Bool      = true,
           outModels :: Bool      = false,
           fitArgs...)</code></pre><p>Cross-validation accuracy for a machine learning <code>model</code>: given an ‚ÑçVector <code>ùêèTr</code> holding <span>$k$</span> Hermitian matrices, an <a href="../MainModule/#IntVector-1">IntVector</a> <code>yTr</code> holding the <span>$k$</span> labels for these matrices and the number of folds <code>nFolds</code>, return a <a href="#PosDefManifoldML.CVacc"><code>CVacc</code></a> structure.</p><p><strong>optional keyword arguments</strong></p><p>The <code>Fisher</code>, <code>logdet0</code> and <code>Wasserstein</code> metric of the machine learning models require an iterative algorithm for computing means in the manifold of positive definite matrices. Argument <code>tol</code> is the tolerance for convergence of these iterative algorithms. In order to speed up computations, set <code>tol</code> to something between 10e-6 (still a fairly good convergence) and 10e-3 (coarse convergence, hence possible drop of classification accuracy, but many less iterations required).</p><p><code>nFolds</code> by default is set to the minimum between 10 and the number of observation √∑ 3 (integer division).</p><p>If <code>scoring</code>=:b (default) the <strong>balanced accuracy</strong> is computed. Any other value will make the function returning the regular <strong>accuracy</strong>. Balanced accuracy is to be preferred for unbalanced classes. For balanced classes the balanced accuracy reduces to the regular accuracy, therefore there is no point in using regular accuracy if not to avoid a few unnecessary computations when the class are balanced.</p><p>For the meaning of the <code>shuffle</code> argument (false by default), see function <a href="#PosDefManifoldML.cvSetup"><code>cvSetup</code></a>, to which this argument is passed.</p><p>Argument <code>vecRange</code> has an effect only for machine learning models in the tangent space. For its meaning see function <a href="#StatsBase.fit"><code>fit</code></a> and <a href="#GLMNet.predict"><code>predict</code></a>, to which it is passed for each fold.</p><p>If <code>verbose</code> is true (default), information is printed in the REPL. This option is included to allow repeated calls to this function without crowding the REPL.</p><p>if <code>outModels</code><code>is true return a 2-tuple holding a [</code>CVacc<code>](@ref) structure and a</code>nFolds<code>-vector of the model fitted for each fold, otherwise (default), return only a [</code>CVacc`](@ref) structure.</p><p><code>fitArgs</code> are optional keyword arguments that are passed to the <a href="#StatsBase.fit"><code>fit</code></a> function called for each fold of the cross-validation. For each machine learning model, all optional keyword arguments of their fit method are elegible to be passed here, however, the arguments listed in the following table for each model should not be passed. Note that if they are passed, they will be disabled:</p><table><tr><th style="text-align: center">MDM/MDMF</th><th style="text-align: center">ENLR</th></tr><tr><td style="text-align: center"><code>verbose</code></td><td style="text-align: center"><code>verbose</code></td></tr><tr><td style="text-align: center"><code>‚è©</code></td><td style="text-align: center"><code>‚è©</code></td></tr><tr><td style="text-align: center"></td><td style="text-align: center"><code>meanISR</code></td></tr><tr><td style="text-align: center"></td><td style="text-align: center"><code>fitType</code></td></tr><tr><td style="text-align: center"></td><td style="text-align: center"><code>offsets</code></td></tr><tr><td style="text-align: center"></td><td style="text-align: center"><code>lambda</code></td></tr><tr><td style="text-align: center"></td><td style="text-align: center"><code>folds</code></td></tr></table><p><strong>See</strong>: <a href="../MainModule/#notation-and-nomenclature-1">notation &amp; nomenclature</a>, <a href="../MainModule/#the-‚ÑçVector-type-1">the ‚ÑçVector type</a>.</p><p><strong>See also</strong>: <a href="#StatsBase.fit"><code>fit</code></a>, <a href="#GLMNet.predict"><code>predict</code></a>.</p><p><strong>Examples</strong></p><pre><code class="language-none">using PosDefManifoldML

# generate some data
PTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80)

# perform 10-fold cross-validation using the minimum distance to mean classifier
cv=cvAcc(MDM(Fisher), PTr, yTr)

# ...using the lasso logistic regression classifier
cv=cvAcc(ENLR(Fisher), PTr, yTr)

# perform 8-fold cross-validation instead
cv=cvAcc(ENLR(Fisher), PTr, yTr; nFolds=8)

# ...using the elastic-net logistic regression (Œ±=0.9) classifier
cv=cvAcc(ENLR(Fisher), PTr, yTr; nFolds=8, alpha=0.9)

# ...and standardizing the predictors
cv=cvAcc(ENLR(Fisher), PTr, yTr; nFolds=8, alpha=0.9, standardize=true)

# perform another cross-validation shuffling the folds
cv=cvAcc(MDM(Fisher), PTr, yTr; nFolds=8, shuffle=true)
</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/Marco-Congedo/PosDefManifoldML.jl/blob/77900b845b66438c11dab22226cf8382ea95afaf/src/cv.jl#L82-L186">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="PosDefManifoldML.cvSetup" href="#PosDefManifoldML.cvSetup"><code>PosDefManifoldML.cvSetup</code></a> ‚Äî <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">function cvSetup(k       :: Int,
                 nCV     :: Int;
                 shuffle :: Bool = false)</code></pre><p>Given <code>k</code> elements and a parameter <code>nCV</code>, a nCV-fold cross-validation is obtained defining <span>$nCV$</span> permutations of <span>$k$</span> elements in <span>$nTest=k√∑nCV$</span> (integer division) elements for the test and <span>$k-nTest$</span> elements for the training, in such a way that each element is represented in only one permutation.</p><p>Said differently, given a length <code>k</code> and the number of desired cross-validations <code>nCV</code>, this function generates indices from the sequence of natural numbers <span>$1,..,k$</span> to obtain all nCV-fold cross-validation sets. Specifically, it generates <span>$nCV$</span> vectors of indices for generating test sets and <span>$nCV$</span> vectors of indices for geerating training sets.</p><p>If optional keyword argument <code>shuffle</code> is true, the sequence of natural numbers <span>$1,..,k$</span> is shuffled before running the function, thus in this case two successive runs of this function will give different cross-validation sets, hence different accuracy scores. By default <code>shuffle</code> is false, so as to allow exactly the same result in successive runs. Note that no random initialization for the shuffling is provided, so as to allow the replication of the same random sequences starting again the random generation from scratch.</p><p>This function is used in <a href="#PosDefManifoldML.cvAcc"><code>cvAcc</code></a>. It constitutes the fundamental basis to implement customized cross-validation procedures.</p><p>Return the 2-tuple with:</p><ul><li>A vector of <code>nCV</code> vectors holding the indices for the training sets,</li><li>A vector of <code>nCV</code> vectors holding the indices for the corresponding test sets.</li></ul><p><strong>Examples</strong></p><pre><code class="language-none">using PosDefManifoldML

cvSetup(10, 2)
# return:
# (Array{Int64,1}[[6, 7, 8, 9, 10], [1, 2, 3, 4, 5]],
#  Array{Int64,1}[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])

cvSetup(10, 2, shuffle=true)
# return:
# (Array{Int64,1}[[5, 4, 6, 1, 9], [3, 7, 8, 2, 10]],
#  Array{Int64,1}[[3, 7, 8, 2, 10], [5, 4, 6, 1, 9]])

cvSetup(10, 3)
# return:
# (Array{Int64,1}[[4, 5, 6, 7, 8, 9, 10], [1, 2, 3, 7, 8, 9, 10], [1, 2, 3, 4, 5, 6]],
#  Array{Int64,1}[[1, 2, 3], [4, 5, 6], [7, 8, 9, 10]])
</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/Marco-Congedo/PosDefManifoldML.jl/blob/77900b845b66438c11dab22226cf8382ea95afaf/src/cv.jl#L316-L373">source</a></section><footer><hr/><a class="previous" href="../enlr/"><span class="direction">Previous</span><span class="title">Elastic-Net Logistic Regression</span></a></footer></article></body></html>
